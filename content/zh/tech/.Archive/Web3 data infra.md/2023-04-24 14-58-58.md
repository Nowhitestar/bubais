---
title: Web3 数据基础设施的混合架构：基于 a16z 文章的探索
date: 2023-04-06T10:50:02+08:00
description:
tags:
- 基础设施
- Web 3.0
- 数据
indent: false
comments: false
toc: false
slug: Web3 data infra
---

ChatGPT 和 GPT-4 的火热，让我们看到了人工智能的力量。人工智能背后，除了算法以外，更重要的是海量的数据。围绕数据，我们已经构建了一个大规模的复杂系统，该系统的价值主要来自于商业智能（Business Intelligence, BI）和人工智能（Artificial Intelligence, AI）。由于互联网时代数据量的快速增长，数据基础设施的工作和最佳实践也在飞速地发展。这两年，数据基础设施技术栈的核心系统已经非常稳定，支持工具和应用也在快速增长。

# Web2 数据基础设施架构

云数据仓库（如 Snowflake 等）正在迅速增长，主要关注 SQL 用户和商业智能用户场景。其他技术的采用也在加速，数据湖（如 Databricks）的客户增长速度前所未有，数据技术栈中的异质性将共存。
其他核心数据系统，如数据获取和转化，已经证明同样耐久。这在现代数据智能领域特别明显。Fivetran 和 dbt（或类似技术）的组合几乎随处可见。但在一定程度上，在业务系统中也同样如此。Databricks/Spark、Confluent/Kafka 和 Astronomer/Airflow 的组合也开始成为事实标准。

![Source: a16z](https://s2.loli.net/2023/04/06/PYoSezxOZdJK8cs.png "Source: a16z")

其中，
- **数据源**端生成相关商务和业务数据；
- **数据抽取和转换**负责从业务系统中抽取数据（E）、传输到存储、对齐数据源和目的地之间的格式（L）以及将分析过的数据根据需求送回业务系统；
- **数据存储**将数据按照可以查询和处理的格式存储，需要朝低成本、高可扩展性和分析工作量进行优化；
- **查询和处理**将高级编程语言（通常用 SQL、Python 或者是 Java/Scala）翻译成低端数据处理任务。根据存储数据，使用分布式计算执行查询和数据模型，包括历史分析（描述过去发生事件）和预测分析（描述将来期待事件）；
- **转换**将数据转换成分析可用的结构，管理流程和资源；
- **分析和输出**是为分析师和数据科学家提供可以溯源洞见和协作的界面，向内部和外部用户展示数据分析的结果，将数据模型嵌入面向用户的应用。

随着数据生态的飞速发展，出现了“数据平台”的概念。从行业的角度看，平台的定义特征是有影响力的平台提供方和大量的第三方开发者能够在技术上和经济上相互依存。从平台的角度看，数据技术栈分为“前端”和“后端”。

**“后端”大致包括数据提取、存储、处理和转换**，已经开始围绕小部分云服务提供商开始整合。因此，客户数据被收集在一套标准的系统中，供应商正在大力投资，使其他开发人员可以轻松访问这些数据。这也是 Databricks 等系统的基本设计原则，并且通过 SQL 标准和自定义计算 API（例如 Snowflake）等系统得到了实现。

“**前端”工程师利用这种单点集成来构建一系列新应用程序。**他们依赖数据仓库/湖仓一体中清洗和整合过的数据，而不用担心它们是如何生成的底层细节。单个客户可以在一个核心数据系统之上构建和购买很多应用。 我们甚至开始看到传统企业系统，如财务或者产品分析，正在使用仓库原生的架构进行重构。

随着数据技术栈的逐渐成熟，数据平台上的数据应用也随之激增。由于标准化，采用新的数据平台变得前所未有地重要，相应地维护平台也变得极为重要。在规模上，平台可能非常有价值。现在，核心数据系统供应商之间竞争激烈，这种竞争不仅是为了当前的业务，更是为了长期的平台地位。**如果你认为数据获取和转换模块是新兴数据平台的核心部分，那么对数据获取和转换公司的惊人估值也就更容易理解了。**

然而，这些技术栈的形成是在以大公司为主导的数据利用方式下形成的。随着社会对于数据的理解加深，人们认为数据与土地、劳动力、资本、技术一样，[都是可市场化配置的生产要素](http://www.gov.cn/zhengce/2020-04/09/content_5500622.htm)。数据作为五大生产要素之一，其背后体现的正是数据的资产价值。

要实现数据要素市场的配置，目前的技术栈远远不能满足需求。与区块链技术紧密结合的 Web3 领域，新的数据基础设施正在发展与演变。这些基础设施将嵌入现代数据基础设施架构，实现**数据产权界定、流通交易、收益分配和要素治理**。这四个领域在[政府监管的角度](http://www.gov.cn/zhengce/2022-12/19/content_5732695.htm) 来说非常关键，因此需要特别关注。

# Web3 混合数据基础设施架构

受 a16z 统一的[数据基础设施架构（2.0）](https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure/)的启发，融合对 Web3 基础设施架构的理解，我们提出了以下 Web3 混合数据基础设施架构。

![](https://s2.loli.net/2023/04/06/XlVc8P5u9IDCpx4.jpg)

橙色是 Web3 所独有的技术栈单元。由于去中心化技术还处于早期发展阶段，目前 Web3 领域内的大部分应用采用的仍是这种混合数据基础设施架构。绝大多数应用并不是真正的“[超级结构](https://jacob.energy/hyperstructures.html)”。超级结构拥有不可停止、免费、有价值、可扩展、无许可、正外部性和可信中立等特征。它作为数字世界的公共物品而存在，是“元宇宙”世界的公共基础设施。这需要完全去中心化的底层架构来支撑它。

传统的数据基础设施架构是根据企业业务发展演变而来的。a16z 将其总结为两个系统（分析系统和业务系统）和三个场景（现代商业智能、多模型数据处理以及人工智能和机器学习）。这是从企业的视角——数据为企业的发展服务——作出的总结。

![Source: a16z](https://s2.loli.net/2023/03/21/sLOo37SQNXn5RDV.png "Source: a16z")

然而，不仅仅是企业，社会和个人都应当受益于数据要素带来的生产力提升。世界各国都接连出台了政策法规，希望从监管的层面规范数据的使用，促进数据的流通。这包括在日本常见的各种 Data Bank、在中国最近兴起的数据交易所以及在欧美已经广泛使用的交易平台，如 BDEX（美国）、Streamr（瑞士）、DAWEX（法国）和 CARUSO 等等。

当数据开始进行产权界定、流动交易、收益分配和治理时，它们的系统和场景就不仅仅是赋能企业自身的决策和业务发展。这些系统和场景要么需要借助区块链技术，要么强烈依赖政策监管。Web3 是数据要素市场的天然土壤，它从技术上杜绝了作弊的可能性，能够大大减轻监管压力，让数据作为真正的生产要素存在，并进行市场化配置。

![](https://s2.loli.net/2023/04/06/d4aSnCHtY6GyDeU.jpg)

在 Web3 语境下，数据利用的新范式包括承载流动数据要素的市场系统和管理公共数据要素的公共系统。它们涵盖了三个新的数据业务场景：产权数据开发整合、可组合初始数据层和公共数据挖掘。
这些场景有的与传统数据基础设施紧密结合，属于 Web3 混合数据基础设施架构；有的则脱离传统架构，完全由 Web3 原生的新技术支持。

# Web3 与数据经济

数据经济市场是配置数据要素的关键，其包括产品数据的开发和整合和具备可组合性的初始数据层市场。在高效合规的数据经济市场中，[以下几点十分重要](http://www.gov.cn/zhengce/2022-12/19/content_5732695.htm)：
1. 数据产权是保障权益和合规使用的关键，应进行结构性分配处置，同时数据使用需要确认授权机制。各个参与方应该拥有相关权益。
2. 流通交易需要场内外结合以及合规高效。应基于数据来源可确认、使用范围可界定、流通过程可追溯、安全风险可防范四大原则。
3. 收益分配制度需要高效公平。按照“谁投入、谁贡献、谁受益”的原则，同时政府在数据要素收益分配中能够发挥引导调节作用。
4. 要素治理安全可控、弹性包容。这需要创新政府数据治理机制，建立数据要素市场信用体系，并鼓励企业积极参与数据要素市场建设，围绕数据来源、数据产权、数据质量、数据使用等，推行面向数据商及第三方专业服务机构的数据流通交易声明和承诺制。

以上原则是监管部门考虑数据经济的基本原则。在产权数据开发整合、可组合初始数据层和公共数据挖掘三种场景下，可以以这些原则为基础进行思考。我们需要怎样的基础设施作为支撑？这些基础设施能够在哪些阶段捕获什么样的价值？

## 场景一：产权数据开发整合

![](https://s2.loli.net/2023/04/06/trBdNm1Z8T9Uw6k.jpg)

在产权数据开发过程中，需要建立**分类分级确权授权机制**，以确定公共数据、企业数据和个人数据的所有权、使用权和经营权。根据数据来源和生成特征，通过“数据适配”的方式对数据进行产权界定。其中，典型的项目包括 Navigate、Streamr Network 和 KYVE 等。这些项目通过技术手段实现数据质量标准化、数据采集和接口标准化，将链下数据以某种形式确权，并通过智能合约或内部逻辑系统进行数据分类分级授权。

在该场景下适用的数据类型为非公共数据，即企业数据和个人数据。应按市场化方式“共同使用、共享收益”，从而激活数据要素价值。
- 企业数据包括各类市场主体在生产经营活动中采集加工的不涉及个人信息和公共利益的数据。市场主体享有依法依规持有、使用、获取收益的权益，以及保障其投入的劳动和其他要素贡献获得合理回报的权利。
- 个人数据要求数据处理者按照个人授权范围依法依规采集、持有、托管和使用数据。使用创新技术手段，推动个人信息匿名化处理，保障使用个人信息数据时的信息安全和个人隐私。探索由受托者代表个人利益，监督市场主体对个人信息数据进行采集、加工、使用的机制。对涉及国家安全的特殊个人信息数据，可依法依规授权有关单位使用。

## 场景二：可组合初始数据层

![](https://s2.loli.net/2023/04/06/K8rnL5ZEhyPuDRV.jpg)

可组合初始数据层是数据经济市场的重要组成部分。与一般的产权数据不同的是，这部分数据最明显的特征是需要通过“数据模式管理”定义数据的标准格式。与“数据适配”的质量、采集和接口标准化不同的是，这里强调的是数据模式的标准化，包括标准的数据格式和标准的数据模型。Ceramic 和 Lens 是这一领域的先行者，他们分别保障了链下（去中心化存储）和链上数据的标准模式，从而使得数据具有可组合性。

搭建在这些数据模式管理工具之上的是可组合初始数据层，通常称为“data layer”，如 Cyberconnect、KNN3 等。

可组合初始数据层较少涉及到 Web2 的技术栈，但以 Ceramic 为主的热数据读取工具打破了这一点，这将是非常关键的突破。很多类似的数据无需存储在区块链上，也很难存储在区块链上，但它们需要存储在去中心化的网络之上，例如用户的发帖、点赞和评论等高频低价值密度数据，Ceramic 为这一类数据提供了存储范式。

可组合的初始数据是新时代创新的关键场景，也是数据霸权与数据垄断终结的重要标志。它[能够解决](https://a16z.com/2019/05/09/data-network-effects-moats/)初创企业在数据方面的冷启动问题，组合成熟数据集和新数据集，从而使初创企业能够更快地建立数据竞争优势。同时让初创企业专注于增量数据价值和数据新鲜度，从而为自身的创新想法赢得持续的竞争力。这样，大量的数据将不会成为大公司的护城河。

## 场景三：公共数据挖掘

![](https://s2.loli.net/2023/04/06/ExU31nwQdTjk57h.jpg)

公共数据挖掘并不是一个新的应用场景，但是在 Web3 技术栈中，它得到了前所未有的突出强调。

传统的公共数据包括党政机关、企事业单位依法履职或提供公共服务过程中产生的公共数据。监管机构鼓励在保护个人隐私和确保公共安全的前提下，按照“原始数据不出域、数据可用不可见”的要求，以模型、核验等产品和服务等形式向社会提供该类数据。它们采用的是传统技术栈（蓝色和部分橙色，橙色代表多个类型技术栈交叉，下同）。

在 Web3 中，区块链上的交易数据以及活动数据则是另一类公共数据，其特征是“可用且可见”，因此缺乏数据隐私、数据安全以及数据使用的确认授权能力，是真正的“公共物品”（Public Goods）。它们采用的是以区块链和智能合约为核心的技术栈（黄色和部分橙色）。

而在去中心化存储上的数据则大多是除交易以外的 Web3 应用数据，目前主要是以文件和对象存储为主，相应的技术栈仍不成熟（绿色和部分橙色）。这类公共数据的生产和挖掘利用存储的普遍问题包括冷热存储、索引、状态同步、权限管理和计算等等。

该场景涌现了诸多数据应用，它们不属于数据基础设施，更多是数据工具，包括 Nansen、Dune、NFTScan、0xScope 等等。

## 案例：数据交易所

数据交易所是指以数据为商品进行交易的平台。它们可以根据交易对象、定价机制、质量保证等方面进行分类和比较。DataStreamX、Dawex、Ocean Protocol 是市场上几个典型的数据交易所。

[Ocean Protocol](https://oceanprotocol.com/) （2亿市值）是一个开源的协议，旨在让企业和个人能够交换和变现数据和基于数据的服务。该协议基于以太坊区块链，使用“数据代币”（datatokens）来控制对数据集的访问。数据代币是一种特殊的 ERC20 代币，可代表一个数据集或一个数据服务的所有权或使用权。用户可以通过购买或赚取数据通证来获取所需的信息。

Ocean Protocol 的技术架构主要包括以下几个部分：
- 提供者（Providers）：指提供数据或数据服务的供应方，他们可以通过 Ocean Protocol 发行和出售自己的数据通证，从而获得收入。
- 消费者（Consumers）：指购买和使用数据或数据服务的需求方，他们可以通过 Ocean Protocol 购买或赚取所需的数据通证，从而获得访问权。
- 市场（Marketplaces）：指由 Ocean Protocol 或第三方提供的一个开放、透明和公平的数据交易市场，它可以连接全球范围内的提供者和消费者，并提供多种类型和领域的数据通证。市场可以帮助组织发现新的商业机会，增加收入来源，优化运营效率，创造更多价值。
- 网络（Network）：指由 Ocean Protocol 提供的一个去中心化的网络层，它可以支持不同类型和规模的数据交换，并保证数据交易过程中的安全、可信和透明。网络层是一组智能合约，用于注册数据、记录所有权信息、促进安全的数据交换等。
- 策展人（Curator）：指一个生态系统中负责筛选、管理、审核数据集的角色，他们负责审核数据集的来源、内容、格式和许可证等方面的信息，以确保数据集符合标准，并且可以被其他用户信任和使用。
- 验证人（Verifier）：指一个生态系统中负责验证、审核数据交易和数据服务的角色，他们对数据服务提供商和消费者之间的交易进行审核和验证，以确保数据服务的质量、可用性和准确性。
- 
![Source: Ocean Protocol](https://s2.loli.net/2022/09/08/NabTRcsoxiSmylW.png "Source: Ocean Protocol")

"Data services" data providers create include data, algorithms, computation, storage, analysis, and curation. These components are tied to the service's execution agreements (service-level agreements), secure computing, access control, and permissions. Essentially, this controls access to a "cloud service suite" through smart contracts.

![Source: Ocean Protocol](https://s2.loli.net/2022/09/08/9AO7PqCBrTIv4tn.png "Source: Ocean Protocol")

The advantage is that,
 - Open-source, flexible, and extensible protocols help organizations and individuals create unique data ecosystems.
 - The decentralized network layer based on blockchain technology can ensure the security, credibility, and transparency of the data transaction process while protecting providers' and consumers' privacy and rights.
 - An open, transparent, and fair data market that can connect providers and consumers worldwide and provide data certificates of various types and fields.

Ocean Protocol is a typical representative of hybrid architecture. Its data can be stored in different places, including traditional cloud storage services, decentralized storage networks, or the data provider's own servers. The protocol uses data tokens (datatokens) and non-homogeneous tokens (data NFTs) to identify and manage data ownership and access rights. In addition, the protocol also provides the function of computing to data (compute-to-data), enabling data consumers to analyze and process data without exposing the original data.

![Source: Ocean Protocol](https://s2.loli.net/2023/03/22/nkgzXBirsUDYtP7.png "Source: Ocean Protocol")

Although Ocean Protocol is one of the complete data trading platforms on the market at this stage, it still faces many challenges:

- Establish an effective trust mechanism to increase the trust between data providers and demanders and reduce transaction risks. For example, establish a data element market credit system, and carry out certification and verification through the blockchain to identify untrustworthy behaviors in data transactions, incentives for keeping promises, punishment for untrustworthiness, credit restoration, objection handling, etc.
 - Establish a reasonable pricing mechanism to reflect the true value of data products, incentivize data providers to provide high-quality data, and attract more demanders.
 - Establish a unified standard specification to facilitate interoperability and compatibility between data of different formats, types, sources, and purposes.

## Case: Data Model Marketplace

Ceramic mentions in their [Data Universe](https://blog.ceramic.network/into-the-dataverse/) the open data model marketplace they want to create because data needs interoperability and it can contribute significantly to productivity gains. Such a data model marketplace is achieved through an urgent consensus on the data model, similar to the ERC contract standard in Ether, from which developers can choose a functional template to have an application that conforms to all the data in that data model. At this stage, such a marketplace is not a trading marketplace.

Regarding the data model, a simple example is that in a decentralized social network, the data model can be reduced to 4 parameters, which are

1. `PostList`: stores the index of user posts
2. `Post`: stores individual posts
3. `Profile`: stores the user's profile
4. `FollowList`: stores the user's follow list

So how can data models be created, shared, and reused on Ceramic to enable cross-application data interoperability?

Ceramic provides a Data Models Registry ([Data Models Registry](https://github.com/ceramicstudio/datamodels)), an open source, community-built, reusable application data model for the Ceramic repository. Here, developers can openly register, discover, and reuse existing data models—the foundation for customer operations applications built on shared data models. Currently, it is based on Github storage, and in the future, it will be decentralized on Ceramic.

All data models added to the registry are automatically published under the `npm` package of **@datamodels**. Any developer can install one or more data models using `@datamodels/model - name`. These models are available for storing or retrieving data at runtime using any IDX client, including DID Data Store or Self.ID.

In addition, Ceramic has built a [Data Models forum](https://github.com/ceramicstudio/datamodels/discussions) based on GitHub. Each model in the data model registry has its own discussion thread on this forum. The community can comment and discuss it. At the same time, it is also a place for developers to publish their ideas on data models so as to obtain the opinions of the community before adding them to the registry. At present, everything is in the early stage. There are not many data models in the registry. The data models included in the registry should be evaluated by the community and called the CIP standard, just like the smart contract standard of Ethereum, which provides data composability.

## Case: Decentralized Data Warehouse

[Space and Time](https://www.spaceandtime.io/) is the first decentralized data warehouse to connect on-chain and off-chain data to support a new generation of smart contract use cases. Space and Time (SxT) has the industry's most mature blockchain indexing service. The SxT data warehouse also employs new cryptography called The SxT data warehouse also employs new cryptography called Proof of SQL™ to generate verifiable, tamper-proof results, allowing developers to join untrusted on-chain and off-chain-data-in a simple SQL format and load the results directly into smart contracts, supporting sub-second queries and enterprise-level analytics in a fully tamper-proof and blockchain-anchored manner.

![Source: Space and Time](https://s2.loli.net/2023/04/06/ncl7ksFVjiKxGUJ.png)

Space and Time is a two-tier network with a validator layer and a data warehouse. the success of the SxT platform depends on the seamless interaction of the validator and the data warehouse to facilitate simple and secure querying of both on-chain and off-chain data.

- A data warehouse consists of database networks and computes clusters controlled by and routed to them by space and time validators. Space and time adopt a flexible storage solution: HTAP (Hybrid transactional/analytic processing).
 - The validator monitors, commands, and validates the services provided by these clusters, then orchestrates the data flow and queries between end users and data warehouse clusters. Validators provide a means for data to enter a system (such as a blockchain index) and data to exit a system (such as a smart contract).
  - Routing - supports transactional and query interactions with a network of decentralized data warehouses
 - Streaming - acts as a sink for high-volume customer streaming (event-driven) workloads
 - Consensus - Provides high-performance Byzantine Fault Tolerance for data entering and exiting the platform
 - Query Proof - provide SQL proof to the platform
 - Table Anchor - provide proof of storage to the platform by anchoring the table on the chain
 - Oracle - supports Web3 interactions, including smart contract event listening and cross-chain messaging/relaying
 - Security - preventing unauthenticated and unauthorized access to the platform

![Source: Space and Time](https://s2.loli.net/2023/04/06/G4C7IYwge89JMtr.png)

Space and Time as a platform is the world's first decentralized data structure, opening up a powerful but underserved market: data sharing. Within the Space and Time platform, companies can share data freely and use smart contracts to trade the shared data. [Additionally, datasets can be proof-of-sql](https://docs.spaceandtime.io/docs/proof-of-sql) monetized and aggregated without giving consumers access to the raw data. Data consumers can trust that aggregations are accurate without seeing the data itself, so data providers no longer have to be data consumers. It is for this reason that the combination of SQL proofs and data structure schemas has the potential to democratize data manipulation, as anyone can contribute to ingesting, transforming, and serving datasets.

# Web3 Data Governance and Discovery

Currently, the Web3 data infrastructure lacks a practical and efficient data governance architecture. However, a practical and efficient data governance infrastructure is essential to configure the data elements of each participant's relevant rights and interests.

- For the data source, it is necessary to have informed consent and the right to acquire, copy and transfer the data itself freely.
- For data processors, they need to have the right to self-control, use data and obtain benefits.
- For data derivatives, operating rights are required.

Currently, Web3 data governance capabilities are single, assets and data (including ceramics) can only be controlled by controlling private keys, and almost no hierarchical classification configuration capability exists. Recently, the innovative mechanisms of Tableland, FEVM, and Greenfield can achieve trustless data governance to a certain extent. Traditional data governance tools such as Collibra can only be used within the enterprise and only have platform-level trust. At the same time, non-decentralized technology also makes it impossible to prevent personal evil and single points of failure. Through data governance tools such as Tableland, the security technologies, standards, and solutions required for the data circulation process can be guaranteed.

## Case: Tableland

Tableland Network is a decentralized web3 protocol for structured relational data, starting with Ethereum (EVM) and EVM-compatible L2. With Tableland, it is now possible to implement traditional web2 relational database functionality by leveraging the blockchain layer for access control. However, Tableland is not a new database **- just web3 native relational tables**.

Tableland offers a new way to enable dapps to store relational data in web3-native networks without these tradeoffs.

**Solutions**

![Source: Tableland](https://s2.loli.net/2023/04/06/hP4jKEHgT3eiInM.png)

With Tableland, metadata can be mutated (with access controls if needed), queried (using familiar SQL), and composable (with other tables on Tableland) - all in a fully decentralized manner.

![Source: Tableland](https://s2.loli.net/2023/04/06/ndTLSx8U6PhrJNj.png)

Tableland breaks down a traditional relational database into two main components: an on-chain registry with access control logic (ACL) and an off-chain (decentralized) table. Every table in Tableland is initially minted as an ERC721 token on a base EVM compatibility layer. Thus, on-chain table owners can set ACL permissions on the table, while off-chain, the Tableland network manages the creation and subsequent alteration of the table itself. Linking between on-chain and off-chain is handled at the contract level. It points to the Tableland network (using base URI + token URI, much like many existing ERC721 tokens that use IPFS gateways or escrow servers for metadata).

![Source: Tableland](https://s2.loli.net/2023/04/06/B8Fkul6f1gsrwyo.png)

Only people with the appropriate on-chain privileges can write to a particular table. However, table reads do not have to be on-chain operations and can use the Tableland gateway; therefore, read queries are free and can come from simple front-end requests or other non-EVM blockchains. To use Tableland, a table must first be created (i.e., cast on the chain as ERC721). The deployment address is initially set to the table owner, and this owner can set permissions for any other users who try to interact with the table to make changes. For example, the owner can set rules for who can update/insert/delete values, what data they can change, and even decide if they want to transfer ownership of the other side of the table. In addition, more complex queries can join data from multiple tables (owned or unowned) to create a fully dynamic and composable relational data layer.

Consider the following diagram, which summarizes a new user's interaction with a table that has been deployed to Tableland by some dapp:

![Source: Tableland](https://s2.loli.net/2023/04/06/8FHr6b1wL3f7ejJ.png)

Here is the overall flow of information:
1. A new user interacts with the dapp's UI and tries to update some information stored in a Tableland table.
2. The dapp calls the Tableland registration smart contract to run this SQL statement, and this contract checks the dapp's smart contract, which contains a custom ACL that defines the permissions of this new user. There are a few points to note:
	1. Custom ACLs in separate smart contracts for dapps are a completely optional but advanced use case; developers don't need to implement custom ACLs and can use the default policy of the Tableland Registry smart contract (only the owner has full permissions).
	2. You can also use [Gateway](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76) to write queries instead of calling Tableland smart contracts directly. There will always be an option for dapps to call Tableland smart contracts directly, but any query can be sent through the [Gateway](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76), which will relay the query to the smart contract on a subsidized basis.
3. The Tableland smart contract takes the user's SQL statements and permissions, incorporating these into emitted events that describe the SQL-based actions to be taken.
4. The Tableland Validator node listens to these events and then takes one of the following actions:
	1. If the user has the correct permissions to write to the table, the validator will run SQL commands accordingly (insert a new row into the table or update an existing value) and broadcast the confirmation data to the Tableland network.
	2. If the user does not have the correct permissions, the Validator will not perform any operations on the table.
	3. If the request is a simple read query, the corresponding data is returned; Tableland is a completely open relational data network where anyone can perform read-only queries on any table.
5. Dapps can reflect any updates on the Tableland network via the [Gateway](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76).

**(Usage scenario) What to avoid**
- **Personally identifiable data -** Tableland is an open network; anyone can read data from any table. Therefore, personal data should not be stored in Tableland.
- **High-frequency, sub-second writes** - e.g., high-frequency trading robots.
- **Storing every user interaction in the application** - keeping this data in Web3 tables, such as keystrokes or clicks, may not make sense. The frequency of writes can lead to high costs.
- **Very large data sets** - These should be avoided and are best handled through file storage, using solutions such as IPFS, Filecoin, or Arweave. However, pointers to these locations and associated metadata are actually a good use case for Tableland tables.

# Thoughts on valuation capture

The different units have an irreplaceable role in the overall data infrastructure architecture, and the value of their value capture is mainly reflected in the market value/valuation and estimated earnings, which can be obtained from the following conclusions:

1. **Data Source** is the module with the largest value capture **in the whole architecture**
2. **Data Replication, Transformation, Streaming, and Data Warehousing** are the next most important
3. **Analytics layer** may have good cash flow, but there will be an upper limit to valuation

Companies/projects on the left of the structure chart tend to capture greater value.

## Industry Concentration

According to incomplete statistical analysis from the companies in the graph above, industry concentration is judged as follows:

1. the highest industry concentration is **data storage** and **data query and processing** two modules
2. the industry concentration is medium is data extraction and conversion
3. the two modules with low industry concentration are data source, analysis, and output

The industry concentration of data sources, analysis, and output is low. The preliminary judgment is that different business scenarios lead to the emergence of vertical scenario leaders in each business scenario, such as Oracle in the database, Stripe in third-party services, Salesforce in enterprise services, Tableau in dashboard analysis, Sisense in embedded analysis, etc.

The reason for the moderate industry concentration of data extraction and conversion modules is initially judged to be due to the technology-oriented nature of the business attributes. The modular form of middleware also makes the switching cost relatively low.

The data storage and query and processing modules with the highest industry concentration are initially judged to be due to the single business scenario, high technical content, high start-up cost, and high cost of subsequent switching, which gives the company/project a strong first-mover advantage and network effect.

## Thoughts on business models and exit paths for data protocols

Judging from the time of establishment and listing,

 - Most companies/projects established before 2010 are data source companies/projects. The mobile Internet has not yet emerged, and the data is not very large. There are also some data storage and analysis output projects, mainly dashboards.
 - From 2010 to 2014, on the eve of the rise of the mobile Internet, data storage and query projects such as Snowflake and Databricks were born, data extraction and conversion projects also began to appear, and a set of mature big data management technology solutions gradually improved. There are a large number of analysis output projects, mainly dashboards.
 - From 2015 to 2020, query and processing projects have sprung up, and many data extraction and conversion projects are also emerging so that people can better exert the power of big data.
 - From 2020 onwards, newer real-time analytics databases and data lake solutions emerged, such as Clickhouse and Tabular.

Infrastructure improvement is the premise of the so-called "mass adoption". During the period of large-scale application, there are still new opportunities. Still, these opportunities are almost only "middleware", and the underlying data warehouse, data source, and other solutions are almost a winner-takes-all situation unless there is a technically substantial Sexual breakthrough. Otherwise, it will be difficult to grow up.

And analysis output projects are opportunities for entrepreneurial projects no matter what period they are in. But it is also constantly iteratively innovating and doing new things based on new scenarios. Tableau, which appeared before 2010, occupied most of the desktop dashboard analysis tools. New scenarios emerged later, such as more professional-oriented DS/ML tools, A more comprehensive data workstation, and a more SaaS-oriented embedded analysis.

Looking at the current data protocol of Web3 from this perspective:

 - The data source and storage projects are not yet determined, but the leaders are emerging. The state storage on the chain is led by Ethereum (220 billion market value), while the decentralized storage is led by Filecoin (2.3 billion market value) and Arweave (280 million market value). There may be a sudden emergence of Greenfield. ——Highest value capture
 - There is still room for innovation in data extraction and conversion projects. The data oracle machine Chainlink (market value of 3.8 billion) is just the beginning. Event streaming and stream processing infrastructure Ceramic and more projects will appear, but there is not much space. - Moderate value capture
 - For query and processing projects, the Graph (with a market value of 1.2 billion) has been able to meet most of the needs, and the type and quantity of projects have not yet reached the explosive period. - Moderate value capture
 - Data analysis projects, mainly Nansen and Dune (valued at 1 billion), need new scenarios for new opportunities. NFT Scan and NFT Go are similar to new scenarios, but they are only content updates, not analysis logic New requirements at the /paradigm level. — Moderate value capture, strong cash flow.

But Web3 is not a copy of Web2 nor a complete evolution of Web2. Web3 has very original missions and scenarios, thus giving birth to business scenarios that are completely different from before (the first three scenarios are all abstractions that can be made now).