---
title: Web3 数据基础设施的混合架构：基于 a16z 文章的探索
date: 2023-04-06T10:50:02+08:00
description:
tags:
- 基础设施
- Web 3.0
- 数据
indent: false
comments: false
toc: false
slug: Web3 data infra
---

ChatGPT 和 GPT-4 的火热，让我们看到了人工智能的力量。人工智能背后，除了算法以外，更重要的是海量的数据。围绕数据，我们已经构建了一个大规模的复杂系统，该系统的价值主要来自于商业智能（Business Intelligence, BI）和人工智能（Artificial Intelligence, AI）。由于互联网时代数据量的快速增长，数据基础设施的工作和最佳实践也在飞速地发展。这两年，数据基础设施技术栈的核心系统已经非常稳定，支持工具和应用也在快速增长。

# Web2 数据基础设施架构

云数据仓库（如 Snowflake 等）正在迅速增长，主要关注 SQL 用户和商业智能用户场景。其他技术的采用也在加速，数据湖（如 Databricks）的客户增长速度前所未有，数据技术栈中的异质性将共存。
其他核心数据系统，如数据获取和转化，已经证明同样耐久。这在现代数据智能领域特别明显。Fivetran 和 dbt（或类似技术）的组合几乎随处可见。但在一定程度上，在业务系统中也同样如此。Databricks/Spark、Confluent/Kafka 和 Astronomer/Airflow 的组合也开始成为事实标准。

![Source: a16z](https://s2.loli.net/2023/04/06/PYoSezxOZdJK8cs.png "Source: a16z")

其中，
- **数据源**端生成相关商务和业务数据；
- **数据抽取和转换**负责从业务系统中抽取数据（E）、传输到存储、对齐数据源和目的地之间的格式（L）以及将分析过的数据根据需求送回业务系统；
- **数据存储**将数据按照可以查询和处理的格式存储，需要朝低成本、高可扩展性和分析工作量进行优化；
- **查询和处理**将高级编程语言（通常用 SQL、Python 或者是 Java/Scala）翻译成低端数据处理任务。根据存储数据，使用分布式计算执行查询和数据模型，包括历史分析（描述过去发生事件）和预测分析（描述将来期待事件）；
- **转换**将数据转换成分析可用的结构，管理流程和资源；
- **分析和输出**是为分析师和数据科学家提供可以溯源洞见和协作的界面，向内部和外部用户展示数据分析的结果，将数据模型嵌入面向用户的应用。

随着数据生态的飞速发展，出现了“数据平台”的概念。从行业的角度看，平台的定义特征是有影响力的平台提供方和大量的第三方开发者能够在技术上和经济上相互依存。从平台的角度看，数据技术栈分为“前端”和“后端”。

**“后端”大致包括数据提取、存储、处理和转换**，已经开始围绕小部分云服务提供商开始整合。因此，客户数据被收集在一套标准的系统中，供应商正在大力投资，使其他开发人员可以轻松访问这些数据。这也是 Databricks 等系统的基本设计原则，并且通过 SQL 标准和自定义计算 API（例如 Snowflake）等系统得到了实现。

“**前端”工程师利用这种单点集成来构建一系列新应用程序。**他们依赖数据仓库/湖仓一体中清洗和整合过的数据，而不用担心它们是如何生成的底层细节。单个客户可以在一个核心数据系统之上构建和购买很多应用。 我们甚至开始看到传统企业系统，如财务或者产品分析，正在使用仓库原生的架构进行重构。

随着数据技术栈的逐渐成熟，数据平台上的数据应用也随之激增。由于标准化，采用新的数据平台变得前所未有地重要，相应地维护平台也变得极为重要。在规模上，平台可能非常有价值。现在，核心数据系统供应商之间竞争激烈，这种竞争不仅是为了当前的业务，更是为了长期的平台地位。**如果你认为数据获取和转换模块是新兴数据平台的核心部分，那么对数据获取和转换公司的惊人估值也就更容易理解了。**

然而，这些技术栈的形成是在以大公司为主导的数据利用方式下形成的。随着社会对于数据的理解加深，人们认为数据与土地、劳动力、资本、技术一样，[都是可市场化配置的生产要素](http://www.gov.cn/zhengce/2020-04/09/content_5500622.htm)。数据作为五大生产要素之一，其背后体现的正是数据的资产价值。

要实现数据要素市场的配置，目前的技术栈远远不能满足需求。与区块链技术紧密结合的 Web3 领域，新的数据基础设施正在发展与演变。这些基础设施将嵌入现代数据基础设施架构，实现**数据产权界定、流通交易、收益分配和要素治理**。这四个领域在[政府监管的角度](http://www.gov.cn/zhengce/2022-12/19/content_5732695.htm) 来说非常关键，因此需要特别关注。

# Web3 混合数据基础设施架构

受 a16z 统一的[数据基础设施架构（2.0）](https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure/)的启发，融合对 Web3 基础设施架构的理解，我们提出了以下 Web3 混合数据基础设施架构。

![](https://s2.loli.net/2023/04/06/XlVc8P5u9IDCpx4.jpg)

橙色是 Web3 所独有的技术栈单元。由于去中心化技术还处于早期发展阶段，目前 Web3 领域内的大部分应用采用的仍是这种混合数据基础设施架构。绝大多数应用并不是真正的“[超级结构](https://jacob.energy/hyperstructures.html)”。超级结构拥有不可停止、免费、有价值、可扩展、无许可、正外部性和可信中立等特征。它作为数字世界的公共物品而存在，是“元宇宙”世界的公共基础设施。这需要完全去中心化的底层架构来支撑它。

传统的数据基础设施架构是根据企业业务发展演变而来的。a16z 将其总结为两个系统（分析系统和业务系统）和三个场景（现代商业智能、多模型数据处理以及人工智能和机器学习）。这是从企业的视角——数据为企业的发展服务——作出的总结。

![Source: a16z](https://s2.loli.net/2023/03/21/sLOo37SQNXn5RDV.png "Source: a16z")

然而，不仅仅是企业，社会和个人都应当受益于数据要素带来的生产力提升。世界各国都接连出台了政策法规，希望从监管的层面规范数据的使用，促进数据的流通。这包括在日本常见的各种 Data Bank、在中国最近兴起的数据交易所以及在欧美已经广泛使用的交易平台，如 BDEX（美国）、Streamr（瑞士）、DAWEX（法国）和 CARUSO 等等。

当数据开始进行产权界定、流动交易、收益分配和治理时，它们的系统和场景就不仅仅是赋能企业自身的决策和业务发展。这些系统和场景要么需要借助区块链技术，要么强烈依赖政策监管。Web3 是数据要素市场的天然土壤，它从技术上杜绝了作弊的可能性，能够大大减轻监管压力，让数据作为真正的生产要素存在，并进行市场化配置。

![](https://s2.loli.net/2023/04/06/d4aSnCHtY6GyDeU.jpg)

在 Web3 语境下，数据利用的新范式包括承载流动数据要素的市场系统和管理公共数据要素的公共系统。它们涵盖了三个新的数据业务场景：产权数据开发整合、可组合初始数据层和公共数据挖掘。
这些场景有的与传统数据基础设施紧密结合，属于 Web3 混合数据基础设施架构；有的则脱离传统架构，完全由 Web3 原生的新技术支持。

# Web3 与数据经济

数据经济市场是配置数据要素的关键，其包括产品数据的开发和整合和具备可组合性的初始数据层市场。在高效合规的数据经济市场中，[以下几点十分重要](http://www.gov.cn/zhengce/2022-12/19/content_5732695.htm)：
1. 数据产权是保障权益和合规使用的关键，应进行结构性分配处置，同时数据使用需要确认授权机制。各个参与方应该拥有相关权益。
2. 流通交易需要场内外结合以及合规高效。应基于数据来源可确认、使用范围可界定、流通过程可追溯、安全风险可防范四大原则。
3. 收益分配制度需要高效公平。按照“谁投入、谁贡献、谁受益”的原则，同时政府在数据要素收益分配中能够发挥引导调节作用。
4. 要素治理安全可控、弹性包容。这需要创新政府数据治理机制，建立数据要素市场信用体系，并鼓励企业积极参与数据要素市场建设，围绕数据来源、数据产权、数据质量、数据使用等，推行面向数据商及第三方专业服务机构的数据流通交易声明和承诺制。

以上原则是监管部门考虑数据经济的基本原则。在产权数据开发整合、可组合初始数据层和公共数据挖掘三种场景下，可以以这些原则为基础进行思考。我们需要怎样的基础设施作为支撑？这些基础设施能够在哪些阶段捕获什么样的价值？

## 场景一：产权数据开发整合

![](https://s2.loli.net/2023/04/06/trBdNm1Z8T9Uw6k.jpg)

在产权数据开发过程中，需要建立**分类分级确权授权机制**，以确定公共数据、企业数据和个人数据的所有权、使用权和经营权。根据数据来源和生成特征，通过“数据适配”的方式对数据进行产权界定。其中，典型的项目包括 Navigate、Streamr Network 和 KYVE 等。这些项目通过技术手段实现数据质量标准化、数据采集和接口标准化，将链下数据以某种形式确权，并通过智能合约或内部逻辑系统进行数据分类分级授权。

在该场景下适用的数据类型为非公共数据，即企业数据和个人数据。应按市场化方式“共同使用、共享收益”，从而激活数据要素价值。
- 企业数据包括各类市场主体在生产经营活动中采集加工的不涉及个人信息和公共利益的数据。市场主体享有依法依规持有、使用、获取收益的权益，以及保障其投入的劳动和其他要素贡献获得合理回报的权利。
- 个人数据要求数据处理者按照个人授权范围依法依规采集、持有、托管和使用数据。使用创新技术手段，推动个人信息匿名化处理，保障使用个人信息数据时的信息安全和个人隐私。探索由受托者代表个人利益，监督市场主体对个人信息数据进行采集、加工、使用的机制。对涉及国家安全的特殊个人信息数据，可依法依规授权有关单位使用。

## 场景二：可组合初始数据层

![](https://s2.loli.net/2023/04/06/K8rnL5ZEhyPuDRV.jpg)

可组合初始数据层是数据经济市场的重要组成部分。与一般的产权数据不同的是，这部分数据最明显的特征是需要通过“数据模式管理”定义数据的标准格式。与“数据适配”的质量、采集和接口标准化不同的是，这里强调的是数据模式的标准化，包括标准的数据格式和标准的数据模型。Ceramic 和 Lens 是这一领域的先行者，他们分别保障了链下（去中心化存储）和链上数据的标准模式，从而使得数据具有可组合性。

搭建在这些数据模式管理工具之上的是可组合初始数据层，通常称为“data layer”，如 Cyberconnect、KNN3 等。

可组合初始数据层较少涉及到 Web2 的技术栈，但以 Ceramic 为主的热数据读取工具打破了这一点，这将是非常关键的突破。很多类似的数据无需存储在区块链上，也很难存储在区块链上，但它们需要存储在去中心化的网络之上，例如用户的发帖、点赞和评论等高频低价值密度数据，Ceramic 为这一类数据提供了存储范式。

可组合的初始数据是新时代创新的关键场景，也是数据霸权与数据垄断终结的重要标志。它[能够解决](https://a16z.com/2019/05/09/data-network-effects-moats/)初创企业在数据方面的冷启动问题，组合成熟数据集和新数据集，从而使初创企业能够更快地建立数据竞争优势。同时让初创企业专注于增量数据价值和数据新鲜度，从而为自身的创新想法赢得持续的竞争力。这样，大量的数据将不会成为大公司的护城河。

## 场景三：公共数据挖掘

![](https://s2.loli.net/2023/04/06/ExU31nwQdTjk57h.jpg)

公共数据挖掘并不是一个新的应用场景，但是在 Web3 技术栈中，它得到了前所未有的突出强调。

传统的公共数据包括党政机关、企事业单位依法履职或提供公共服务过程中产生的公共数据。监管机构鼓励在保护个人隐私和确保公共安全的前提下，按照“原始数据不出域、数据可用不可见”的要求，以模型、核验等产品和服务等形式向社会提供该类数据。它们采用的是传统技术栈（蓝色和部分橙色，橙色代表多个类型技术栈交叉，下同）。

在 Web3 中，区块链上的交易数据以及活动数据则是另一类公共数据，其特征是“可用且可见”，因此缺乏数据隐私、数据安全以及数据使用的确认授权能力，是真正的“公共物品”（Public Goods）。它们采用的是以区块链和智能合约为核心的技术栈（黄色和部分橙色）。

而在去中心化存储上的数据则大多是除交易以外的 Web3 应用数据，目前主要是以文件和对象存储为主，相应的技术栈仍不成熟（绿色和部分橙色）。这类公共数据的生产和挖掘利用存储的普遍问题包括冷热存储、索引、状态同步、权限管理和计算等等。

该场景涌现了诸多数据应用，它们不属于数据基础设施，更多是数据工具，包括 Nansen、Dune、NFTScan、0xScope 等等。

## 案例：数据交易所

数据交易所是指以数据为商品进行交易的平台。它们可以根据交易对象、定价机制、质量保证等方面进行分类和比较。DataStreamX、Dawex、Ocean Protocol 是市场上几个典型的数据交易所。

[Ocean Protocol](https://oceanprotocol.com/) （2亿市值）是一个开源的协议，旨在让企业和个人能够交换和变现数据和基于数据的服务。该协议基于以太坊区块链，使用“数据代币”（datatokens）来控制对数据集的访问。数据代币是一种特殊的 ERC20 代币，可代表一个数据集或一个数据服务的所有权或使用权。用户可以通过购买或赚取数据通证来获取所需的信息。

Ocean Protocol 的技术架构主要包括以下几个部分：
- 提供者（Providers）：指提供数据或数据服务的供应方，他们可以通过 Ocean Protocol 发行和出售自己的数据通证，从而获得收入。
- 消费者（Consumers）：指购买和使用数据或数据服务的需求方，他们可以通过 Ocean Protocol 购买或赚取所需的数据通证，从而获得访问权。
- 市场（Marketplaces）：指由 Ocean Protocol 或第三方提供的一个开放、透明和公平的数据交易市场，它可以连接全球范围内的提供者和消费者，并提供多种类型和领域的数据通证。市场可以帮助组织发现新的商业机会，增加收入来源，优化运营效率，创造更多价值。
- 网络（Network）：指由 Ocean Protocol 提供的一个去中心化的网络层，它可以支持不同类型和规模的数据交换，并保证数据交易过程中的安全、可信和透明。网络层是一组智能合约，用于注册数据、记录所有权信息、促进安全的数据交换等。
- 策展人（Curator）：指一个生态系统中负责筛选、管理、审核数据集的角色，他们负责审核数据集的来源、内容、格式和许可证等方面的信息，以确保数据集符合标准，并且可以被其他用户信任和使用。
- 验证人（Verifier）：指一个生态系统中负责验证、审核数据交易和数据服务的角色，他们对数据服务提供商和消费者之间的交易进行审核和验证，以确保数据服务的质量、可用性和准确性。
- 
![Source: Ocean Protocol](https://s2.loli.net/2022/09/08/NabTRcsoxiSmylW.png "Source: Ocean Protocol")

数据提供者创建的“数据服务”包括数据、算法、计算、存储、分析和策展。这些组件与服务的执行协议（如服务等级协议）、安全计算、访问控制和许可绑定在一起。本质上，这是通过智能合约来控制一个“云服务套件”的访问权限。
![Source: Ocean Protocol](https://s2.loli.net/2022/09/08/9AO7PqCBrTIv4tn.png "Source: Ocean Protocol")

其优点是，
- 开源、灵活和可扩展的协议有助于组织和个人创建自己独特的数据生态系统。
- 基于区块链技术的去中心化网络层，可以保证数据交易过程中的安全、可信和透明，同时也保护了提供者和消费者的隐私和权益。
- 开放、透明和公平的数据市场，可以连接全球范围内的提供者和消费者，并提供多种类型和领域的数据通证。

Ocean Protocol 是混合架构的典型代表。其数据可以存储在不同的地方，包括传统的云存储服务、去中心化的存储网络，或者数据提供者自己的服务器。该协议通过数据代币（datatokens）和数据非同质化代币（data NFTs）来标识和管理数据的所有权和访问权限。此外，该协议还提供了计算到数据（compute-to-data）的功能，使得数据消费者可以在不暴露原始数据的情况下对数据进行分析和处理。

![Source: Ocean Protocol](https://s2.loli.net/2023/03/22/nkgzXBirsUDYtP7.png "Source: Ocean Protocol")

固然 Ocean Protocol 是市面上现阶段最为完善的数据交易平台之一，但它仍然面临着诸多挑战：
- 建立一个有效的信任机制，以增加数据提供者和需求者之间的信任度，降低交易风险。例如，建立数据要素市场信用体系，对数据交易失信行为认定、守信激励、失信惩戒、信用修复、异议处理等等，通过区块链进行留证与验证。
- 建立一个合理的定价机制，以反映数据产品的真实价值，激励数据提供者提供高质量的数据，并吸引更多的需求者。
- 建立一个统一的标准规范，以促进不同格式、类型、来源和用途的数据之间的互操作性和兼容性。

## 案例：数据模型市场

Ceramic 在其[数据宇宙](https://blog.ceramic.network/into-the-dataverse/)中提到了他们要打造的开放数据模型市场，因为数据需要互操作性，它能够极大地促进生产力的提升。这样的数据模式市场是通过对数据模型的紧急共识实现的，就类似于以太坊中的 ERC 合约标准，开发人员可以从中选择作为功能模板，从而拥有一个符合该数据模型的所有数据的应用程序。目前这个阶段，这样的市场并不是一个交易市场。

关于数据模型，一个简单的例子是，在去中心化社交网络当中，数据模型可以简化为 4 个参数，分别是：
1. `PostList`：存储用户帖子的索引
2. `Post`：存储单个帖子
3. `Profile`：存储用户的资料
4. `FollowList`：存储用户的关注列表

那么数据模型如何在 Ceramic 上进行创建、共享和重用，从而实现跨应用程序数据互操作性呢？

Ceramic 提供了一个数据模型注册表（[DataModels Registry](https://github.com/ceramicstudio/datamodels)），这是一个开源的、社区共建的、用于 Ceramic 的可重用应用程序数据模型的存储库。在这里，开发人员可以在其中公开注册、发现和重用现有数据模型——这是构建在共享数据模型上的客户操作应用程序的基础。目前，它基于 Github 存储，未来它将分散在 Ceramic 上。

添加到注册表的所有数据模型都会自动发布到 `@datamodels` 的 npm 插件包下面。任何开发人员都可以使用 `@datamodels/model-name` 安装一个或多个数据模型，使这些模型可用于在运行时使用任何 IDX 客户端存储或检索数据，包括 DID DataStore 或 Self.ID。

此外，Ceramic 还基于 Github 搭建了一 个[DataModels](https://github.com/ceramicstudio/datamodels/discussions) 论坛 ，数据模型注册表中的每个模型在该论坛上都有自己的讨论线程，社区可以通过它来评论和讨论。同时，这里还可以供开发人员发布关于数据模型的想法，从而在将其添加到注册表之前征求社区的意见。目前一切都在早期阶段，注册表中的数据模型并不多，收纳进入注册表中的数据模型应当通过社区的评定称为 CIP 标准，就像以太坊的智能合约标准一样，这为数据提供了可组合性。

## 案例：去中心化数据仓库

[Space and Time](https://www.spaceandtime.io/) 是第一个连接链上和链下数据以支持新一代智能合约用例的去中心化数据仓库。Space and Time (SxT) 拥有业内最成熟的区块链索引服务，SxT 数据仓库还采用了一种名为 Proof of SQL™ 的新型密码学来生成可验证的防篡改结果，允许开发人员以简单的 SQL 格式加入无需信任的链上和链下数据，并将结果直接加载到智能合约中，以完全防篡改和区块链锚定的方式为亚秒级查询和企业级分析提供支持。

![Source: Space and Time](https://s2.loli.net/2023/04/06/ncl7ksFVjiKxGUJ.png "Source: Space and Time")

Space and Time 是两层网络，由验证器层和数据仓库组成。SxT 平台的成功取决于验证器和数据仓库的无缝交互，以促进对链上和链下数据的简单和安全查询。

- 数据仓库由数据库网络和计算集群组成，这些网络由 space and time 验证器控制并路由到它们。Space and time 采用了一种非常灵活的仓储解决方案：HTAP（Hybrid transactional/analytic processing）。
- Validator 监视、命令和验证这些集群提供的服务，然后编排最终用户和数据仓库集群之间的数据流和查询。Validator 为数据进入系统（例如区块链索引）和数据退出系统（例如智能合约）提供了一种手段。
  - 路由——支持与去中心化数据仓库网络的事务和查询交互
  - 流媒体——充当大容量客户流媒体（事件驱动）工作负载的接收器
  - 共识——对进出平台的数据提供高性能的拜占庭容错
  - 查询证明——向平台提供 SQL 证明
  - Table Anchor——通过在链上锚定表向平台提供存储证明
  - Oracle——支持 Web3 交互，包括智能合约事件监听和跨链消息传递/中继
  - 安全性——防止未经身份验证和未经授权访问平台

![Source: Space and Time](https://s2.loli.net/2023/04/06/G4C7IYwge89JMtr.png "Source: Space and Time")

Space and Time 作为一个平台是世界上第一个分散的数据结构，它开启了一个强大但服务不足的市场：数据共享。在 Space and Time 平台内，公司可以自由共享数据，并且可以使用智能合约对共享的数据进行交易。此外，[数据集可以通过SQL 证明](https://docs.spaceandtime.io/docs/proof-of-sql)以聚合方式货币化，而无需让消费者访问原始数据。数据消费者可以相信聚合是准确的，而无需看到数据本身，因此数据提供者不再必须是数据消费者。正是出于这个原因，SQL 证明和数据结构架构的结合有可能使数据操作民主化，因为任何人都可以在摄取、转换和服务数据集方面做出贡献。

# Web3 数据治理与发现

目前，Web3 数据基础设施架构中缺乏一个实用且高效的数据治理架构。然而，一个实用且高效的数据治理基础设施对于配置各参与方相关权益的数据要素至关重要。
- 对于数据来源者，需要拥有知情同意和数据本身的自由获取、复制转移的处置权。
- 对于数据处理者，需要拥有自主管控、使用数据和获得收益的权力。
- 对于数据衍生品，需要拥有经营权。

目前 Web3 数据治理能力单一，往往只能通过控制私钥来控制资产和数据（包括 Ceramic），分级分类配置能力几乎没有。最近，Tableland、FEVM 以及 Greenfield 的创新机制，在一定程度上可以实现数据的去信任化治理。传统的数据治理工具如 Collibra 一般只能用于企业内部，只具备平台级的信任，同时非去中心化的技术也使得其无法防止个人作恶及单点故障。通过 Tableland 等数据治理工具，可以保障数据流通过程所需的安全保障技术、标准和方案。

## 案例：Tableland

Tableland Network 是一种用于结构化关系数据的分散式 web3 协议，从以太坊 (EVM) 和与 EVM 兼容的 L2 开始。借助 Tableland，现在可以通过利用区块链层进行访问控制来实现传统的 web2 关系数据库功能。但是，Tableland 并不是一个新的数据库**——它只是 web3 原生的关系表**。
Tableland 提供了一种新方法，使 dapp 能够将关系数据存储在 web3-native 网络中，而无需进行这些权衡。

### 解决方案

![Source: Tableland](https://s2.loli.net/2023/04/06/hP4jKEHgT3eiInM.png "Source: Tableland")

使用 Tableland，元数据可以变更（如果需要，使用访问控制）、查询（使用熟悉的 SQL）和可组合（与 Tableland 上的其他表）——所有这些都以完全去中心化的方式进行。

![Source: Tableland](https://s2.loli.net/2023/04/06/ndTLSx8U6PhrJNj.png "Source: Tableland")

使用 Tableland，元数据可以变更（如果需要，使用访问控制）、查询（使用熟悉的 SQL）和可组合（与 Tableland 上的其他表）——所有这些都以完全去中心化的方式进行。

![Source: Tableland](https://s2.loli.net/2023/04/06/B8Fkul6f1gsrwyo.png "Source: Tableland")

只有具有适当链上权限的人才能写入特定表。但是，表读取不一定是链上操作，可以使用 Tableland 网关；因此，读取查询是免费的，可以来自简单的前端请求，甚至可以来自其他非 EVM 区块链。现在，为了使用 Tableland，必须首先创建一个表（即，作为 ERC721 在链上铸造）。部署地址最初设置为表所有者，并且此所有者可以为任何其他尝试与表交互进行变更的用户设置权限。例如，所有者可以设置规则，谁可以更新/插入/删除值，他们可以更改哪些数据，甚至决定他们是否愿意转让所有权表的另一方。此外，更复杂的查询可以连接来自多个表（拥有或非拥有）的数据，以创建一个完全动态且可组合的关系数据层。

考虑下图，它概括了新用户与已由某些 dapp 部署到 Tableland 的表的交互：

![Source: Tableland](https://s2.loli.net/2023/04/06/8FHr6b1wL3f7ejJ.png "Source: Tableland")

以下是整体信息流：
1. 新用户与 dapp 的 UI 交互并尝试更新存储在 Tableland 表中的一些信息。
2. dapp 调用 Tableland 注册智能合约来运行这个 SQL 语句，并且这个合约检查 dapp 的智能合约，其中包含定义这个新用户的权限的自定义 ACL。有几点需要注意：
    1. dapp 的单独智能合约中的自定义 ACL是一个完全可选但高级的用例；开发人员不需要实现自定义 ACL，可以使用 Tableland 注册表智能合约的默认策略（只有所有者拥有完全权限）。
    2. 写查询也可以使用[网关](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76)，而不是直接调用 Tableland 智能合约。dapp 始终存在直接调用 Tableland 智能合约的选项，但任何查询都可以通过网关发送，网关将以补贴的方式将查询中继到智能合约本身。
3. Tableland 智能合约获取该用户的 SQL 语句和权限，并将这些合并到发出的事件中，这些事件描述了要采取的基于 SQL 的操作。
4. Tableland Validator 节点侦听这些事件并随后采取以下操作之一：
    1. 如果用户具有写入表的正确权限，验证器将相应地运行 SQL 命令（例如，向表中插入新行或更新现有值）并将确认数据广播到 Tableland 网络。
    2. 如果用户没有正确的权限，Validator不会对表执行任何操作。
    3. 如果请求是简单的读查询，则返回相应的数据；Tableland 是一个完全开放的关系数据网络，任何人都可以在其中对任何表执行只读查询。
5. dapp 将能够通过网关反映 Tableland 网络上发生的任何更新。

Here is the overall flow of information:
1. A new user interacts with the dapp's UI and tries to update some information stored in a Tableland table.
2. The dapp calls the Tableland registration smart contract to run this SQL statement, and this contract checks the dapp's smart contract, which contains a custom ACL that defines the permissions of this new user. There are a few points to note:
	1. Custom ACLs in separate smart contracts for dapps are a completely optional but advanced use case; developers don't need to implement custom ACLs and can use the default policy of the Tableland Registry smart contract (only the owner has full permissions).
	2. You can also use [Gateway](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76) to write queries instead of calling Tableland smart contracts directly. There will always be an option for dapps to call Tableland smart contracts directly, but any query can be sent through the [Gateway](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76), which will relay the query to the smart contract on a subsidized basis.
3. The Tableland smart contract takes the user's SQL statements and permissions, incorporating these into emitted events that describe the SQL-based actions to be taken.
4. The Tableland Validator node listens to these events and then takes one of the following actions:
	1. If the user has the correct permissions to write to the table, the validator will run SQL commands accordingly (insert a new row into the table or update an existing value) and broadcast the confirmation data to the Tableland network.
	2. If the user does not have the correct permissions, the Validator will not perform any operations on the table.
	3. If the request is a simple read query, the corresponding data is returned; Tableland is a completely open relational data network where anyone can perform read-only queries on any table.
5. Dapps can reflect any updates on the Tableland network via the [Gateway](https://docs.tableland.xyz/faqs#4dcbf05f97a74829a0ba6d5bb6ed1a76).

**(Usage scenario) What to avoid**
- **Personally identifiable data -** Tableland is an open network; anyone can read data from any table. Therefore, personal data should not be stored in Tableland.
- **High-frequency, sub-second writes** - e.g., high-frequency trading robots.
- **Storing every user interaction in the application** - keeping this data in Web3 tables, such as keystrokes or clicks, may not make sense. The frequency of writes can lead to high costs.
- **Very large data sets** - These should be avoided and are best handled through file storage, using solutions such as IPFS, Filecoin, or Arweave. However, pointers to these locations and associated metadata are actually a good use case for Tableland tables.

# Thoughts on valuation capture

The different units have an irreplaceable role in the overall data infrastructure architecture, and the value of their value capture is mainly reflected in the market value/valuation and estimated earnings, which can be obtained from the following conclusions:

1. **Data Source** is the module with the largest value capture **in the whole architecture**
2. **Data Replication, Transformation, Streaming, and Data Warehousing** are the next most important
3. **Analytics layer** may have good cash flow, but there will be an upper limit to valuation

Companies/projects on the left of the structure chart tend to capture greater value.

## Industry Concentration

According to incomplete statistical analysis from the companies in the graph above, industry concentration is judged as follows:

1. the highest industry concentration is **data storage** and **data query and processing** two modules
2. the industry concentration is medium is data extraction and conversion
3. the two modules with low industry concentration are data source, analysis, and output

The industry concentration of data sources, analysis, and output is low. The preliminary judgment is that different business scenarios lead to the emergence of vertical scenario leaders in each business scenario, such as Oracle in the database, Stripe in third-party services, Salesforce in enterprise services, Tableau in dashboard analysis, Sisense in embedded analysis, etc.

The reason for the moderate industry concentration of data extraction and conversion modules is initially judged to be due to the technology-oriented nature of the business attributes. The modular form of middleware also makes the switching cost relatively low.

The data storage and query and processing modules with the highest industry concentration are initially judged to be due to the single business scenario, high technical content, high start-up cost, and high cost of subsequent switching, which gives the company/project a strong first-mover advantage and network effect.

## Thoughts on business models and exit paths for data protocols

Judging from the time of establishment and listing,

 - Most companies/projects established before 2010 are data source companies/projects. The mobile Internet has not yet emerged, and the data is not very large. There are also some data storage and analysis output projects, mainly dashboards.
 - From 2010 to 2014, on the eve of the rise of the mobile Internet, data storage and query projects such as Snowflake and Databricks were born, data extraction and conversion projects also began to appear, and a set of mature big data management technology solutions gradually improved. There are a large number of analysis output projects, mainly dashboards.
 - From 2015 to 2020, query and processing projects have sprung up, and many data extraction and conversion projects are also emerging so that people can better exert the power of big data.
 - From 2020 onwards, newer real-time analytics databases and data lake solutions emerged, such as Clickhouse and Tabular.

Infrastructure improvement is the premise of the so-called "mass adoption". During the period of large-scale application, there are still new opportunities. Still, these opportunities are almost only "middleware", and the underlying data warehouse, data source, and other solutions are almost a winner-takes-all situation unless there is a technically substantial Sexual breakthrough. Otherwise, it will be difficult to grow up.

And analysis output projects are opportunities for entrepreneurial projects no matter what period they are in. But it is also constantly iteratively innovating and doing new things based on new scenarios. Tableau, which appeared before 2010, occupied most of the desktop dashboard analysis tools. New scenarios emerged later, such as more professional-oriented DS/ML tools, A more comprehensive data workstation, and a more SaaS-oriented embedded analysis.

Looking at the current data protocol of Web3 from this perspective:

 - The data source and storage projects are not yet determined, but the leaders are emerging. The state storage on the chain is led by Ethereum (220 billion market value), while the decentralized storage is led by Filecoin (2.3 billion market value) and Arweave (280 million market value). There may be a sudden emergence of Greenfield. ——Highest value capture
 - There is still room for innovation in data extraction and conversion projects. The data oracle machine Chainlink (market value of 3.8 billion) is just the beginning. Event streaming and stream processing infrastructure Ceramic and more projects will appear, but there is not much space. - Moderate value capture
 - For query and processing projects, the Graph (with a market value of 1.2 billion) has been able to meet most of the needs, and the type and quantity of projects have not yet reached the explosive period. - Moderate value capture
 - Data analysis projects, mainly Nansen and Dune (valued at 1 billion), need new scenarios for new opportunities. NFT Scan and NFT Go are similar to new scenarios, but they are only content updates, not analysis logic New requirements at the /paradigm level. — Moderate value capture, strong cash flow.

But Web3 is not a copy of Web2 nor a complete evolution of Web2. Web3 has very original missions and scenarios, thus giving birth to business scenarios that are completely different from before (the first three scenarios are all abstractions that can be made now).